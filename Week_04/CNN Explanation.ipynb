{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a CNN?\n",
    "### Convolutional Neural Network\n",
    "\n",
    "This is a neural network, which is responsible for taking in images and doing something with them.\n",
    "\n",
    "Think neural networks, but for computer vision.\n",
    "\n",
    "Very complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is the general form of a convolutional neural network.\n",
    "#### We will dive into this later. Just keep this in mind\n",
    "\n",
    "![](images/1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolution is where you apply a filter to a single position of the input. This will be used to build a convolutional unit, which\n",
    "* Takes an input volume\n",
    "* Applies a filter at every position of the input\n",
    "* Outputs another volume (of different size, usually)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/Convolution_schematic.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a computer vision application, each value in the matrix on the left corresponds to a single pixel value, and we convolve a 3x3 filter with the image by multiplying its values element-wise with the original matrix, then summing them up.\n",
    "\n",
    "## Forward Pass\n",
    "In the forward pass, you will take many filters and convolve them on the input. Each 'convolution' gives you a 2D matrix output. You will then stack these outputs to get a 3D volume.\n",
    "\n",
    "## Padding:\n",
    "### Zero Padding:\n",
    "\n",
    "![](images/3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Pooling layer\n",
    "\n",
    "The pooling (POOL) layer reduces the height and width of the input. It helps reduce computation further down the line, and also helps to make feature detectors more invariant to its position in the input. There are two types of pooling layers.\n",
    "\n",
    "* Max pooling - slides an (f,f) window over the input and stores the max value of the window in the output\n",
    "* Averahe pooling - slides an (f,f) window over the input and stores the average value of the window in the output\n",
    "\n",
    "![](images/pool_1.PNG)\n",
    "![](images/pool_2.PNG)\n",
    "\n",
    "These pooling layers have no parameters for backpropagation to train. However, they have hyperparameters, such as the window size f. This specifies the height and width of the fxf window you would compute a max or average over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout?\n",
    "\n",
    "Dropout is a regularization technique(un-overfitting the model) for neural networks of all types\n",
    "Using “dropout\", you randomly deactivate certain units (neurons) in a layer with a certain probability p from a Bernoulli distribution (typically 50%, but this yet another hyperparameter to be tuned). So, if you set half of the activations of a layer to zero, the neural network won’t be able to rely on particular activations in a given feed-forward pass during training. As a consequence, the neural network will learn different, redundant representations; the network can’t rely on the particular neurons and the combination (or interaction) of these to be present. Another nice side effect is that training will be faster.\n",
    "\n",
    "Additional technical notes: Dropout is only applied during training, and you need to rescale the remaining neuron activations. E.g., if you set 50% of the activations in a given layer to zero, you need to scale up the remaining ones by a factor of 2. Finally, if the training has finished, you’d use the complete network for testing (or in other words, you set the dropout probability to 0).\n",
    "\n",
    "![](images/dropout.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense\n",
    "\n",
    "In contrast to the dropout layer, a dense layer is simply a layer where each unit or neuron is connected to each neuron in the next layer (fully connected layer)\n",
    "\n",
    "Each output node is the function of __all__ the input nodes.\n",
    "\n",
    "These are usually used in the end as an output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In summary:\n",
    "\n",
    "Dense layer: A linear operation in which every input is connected to every output by a weight (so there are n_inputs * n_outputs weights - which can be a lot!). Generally followed by a non-linear activation function\n",
    "\n",
    "\n",
    "Convolutional layer: A linear operation using a subset of the weights of a dense layer. Nearby inputs are connected to nearby outputs (specifically - a convolution). The weights for the convolutions at each location are shared. Due to the weight sharing, and the use of a subset of the weights of a dense layer, there’s far less weights than in a dense layer. Generally followed by a non-linear activation function\n",
    "\n",
    "\n",
    "Pooling layer: Replace each patch in the input with a single output, which is the maximum (can also be average) of the input patch. Typically halves the size of the image (2x2 filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation in CNNs\n",
    "A whole lotta calculus.\n",
    "\n",
    "As long as you understand how backpropagation works in normal neural networks, you can skip this. =)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
